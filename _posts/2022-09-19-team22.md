---
layout: post
comments: true
title: Self-Play for Competetive Multi-Agent Gameplay
author: Ben Klingher, Erik Ren (Team 22)
date: 2022-10-19
---


## Summary
Self-play is a form of multi-agent reinforcment learning used for competitive game enviornments, where the agent is trained by competing against different versions of itself in order to improve performance. We intend to build a reinforcment learning system capable of training agents that compete successfully in multiple two-player competetive environments.

## Environment

We will train a competitve agent in one environment: Pong, using either the environments impemented in [this repo](https://github.com/ucla-rlcourse/competitive-rl), or the OpenAI gym Atari version of Pong.

Note that we dropped car racing because it is not a good candidate for self-play. The result of each racer gets a score which means it is not evaluated as much by direct competition, and therefore the training will not benefit from the self-play model.

## Technique

As part of development, we plan to try out several approaches. We will begin by focusing on deep Q-learning. When we have developed a successful agent, we will experiment with direct competition between agent and the default agents in the environments. As time permits, we may deploy the self-play system to AWS Elastic Compute to accelerate the training.

## Evaluation

To measure the performance of our trained agent, we plan to compare the win-rate of our model to a set of control agents. For example, the pong environment we use features 6 pre-trained agents ranked from random, rule-base, weak, medium, strong, and alpha. We will evaluate the win-rate for each of the control agents against our trained agent to determine both progress and success. We apply a similar evaluation method to all games played with the goal of reaching a positive win-rate.

## Mid-Quarter Update

So far, we have focused on training a DQN model for Pong. We experimented with both the environment provided by the course as well as the one in OpenAI gym. While we were able to adapt the DQN model from the homework to the OpenAI Pong environment using a convolutional neural network, we ran into significant issues in training. It would take well over 8 hours to train and still not be effective. In our research, we have discovered that self-play is by design a compute heavy task and, therefore, we may be quite limited by that bottleneck.

As a result, we have decided to make several adjustments to our strategy. First, instead of trying to build our DQN model from scratch we will find an efficient implementation designed to fit an Atari-like environment (perhaps Pong itself). This will allow us to skip the tedious step of assembling our algorithm and move to the core part of our project which is the implementation of iterative self play. Second, we have discovered several implementations that do significant preprocessing of the environment to simplify the state space as much as possible before training. We will employ several of these strategies. Third, until now we have been running the training locally on our own laptops. This is a significant limitation in compute resources. We plan to deploy to EC2 or GCE in order to both allow for easier long runs for training and to possibly have access to GPU and TPU resources to see if we can enhance efficiency. Finally, as we learned in the last few weeks of class, some RL algorithms converge far more quickly in training than others. We will experiment with the deep RL algorithm that is best suited to quick training for a pixel-grid environment with a convolutional neural network.

Also, in addition to building a DQN agent for playing pong, we have also explored the competitive pong environment (cpong-v0) to train and evaluate our agents. This environment allows us to control both players (left and right) so we can explore the behavior of two similar agents in a competitive context. Currently, we have built a demo environment that uses pre-trained agents to compete against each other. We will eventually develop our own DQN agent (or any efficient implementation) and integrate it into the existing competitive pong environment to perform experiments against itself. We plan to use this environment to investigate the performance of self-trained competitive agents compared to those that are trained against traditional non-adaptive agents. This follows from a 2017 Multiagent deep reinforcement learning paper (the first listed below) that investigated the behavior of cooperative and competitive agents using DQN in Pong. After completing these steps, we will explore the emergence of competitive behavior and compare the strategies used by these agents when trained against another adaptive agent versus a hard-wired algorithm.


## References

### Papers we discovered with further research:

Multiagent cooperation and competition with deep reinforcement learning:
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0172395

**^^ this one focuses on competitive Pong and DQN**

Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality:
https://proceedings.neurips.cc/paper/2021/file/dd1970fb03877a235d530476eb727dab-Paper.pdf

Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments:
https://proceedings.neurips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf

Emergent Complexity via Multi-Agent Competition:
https://arxiv.org/pdf/1710.03748.pdf

Improving Q-Learning with Functional Regularization:
https://arxiv.org/pdf/2106.02613.pdf

### Initial papers we looked at:

Emergent Complexity via Multi-Agent Competition
https://arxiv.org/pdf/1710.03748.pdf

Human-level control through deep reinforcement learning
https://www.nature.com/articles/nature14236

Deep Reinforcement Learning with Double Q-learning
https://arxiv.org/abs/1509.06461

Competitive Multi-Agent Reinforcement Learning with Self-Supervised Representation
https://ieeexplore.ieee.org/document/9747378